---
title: "Taller 1 Multivariado"
output:
  html_document:
    toc: true         # Activa la barra de navegación
    toc_float: true   # Flota a medida que te desplazas
    toc_depth: 3 
  pdf_document: default
author: "Juan Andrés Camacho, Camilo Alejandro Raba  \nSebastian Orlando Olarte, Camilo
  Esteban Gomez\n"

date: "2024-11-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.show = "hold")
```

## Taller 1

Del dataset Paper-Quality Measurements presentado por Johnson & Wichern (2014) en la Tabla 1.2, página 15, encuentre e interprete:

```{r, warning=FALSE, message=FALSE}
library(readr)
library(sqldf)
library(DepthProc)
library(MASS)
library(knitr)
library(kableExtra)
library(MVN) 
library(aplpack)
library(ggplot2)
library(readxl)
PQM=read.csv("PQM.csv")
PQM=PQM[,-1]
n=nrow(PQM) #Número de observaciones
p=ncol(PQM) #Número de variables

kable(PQM)%>%
  kable_styling() %>%
  scroll_box(height="500px")
```

## 1. El vector de medias muestrales usando los vectores observados y la matriz de datos.

Para calcular el vector de medias muestrales usando los vectores observados es necesario realizar la siguiente operación: $\overline{\mathbf{x}}=\frac{1}{n} \sum_{j=1}^n \mathbf{x}_j$
Es decir, debemos sumar las observaciones sobre todos los individuos y dividir esa suma por el número total de individuos.

```{r, warning=FALSE, message=FALSE}
mean_v=as.vector(rep(0,p))
 
for (i in c(1:n)){ #Para realizar la suma sobre los individuos
  a = PQM[i,]
  mean_v=mean_v+a
}
mean_v=(1/n)*mean_v
mean_v=t(as.matrix(mean_v))
kable(mean_v)
```

El calculo del vector de medias a través de la matriz de datos se representa matricialmente de la siguiente manera: $\overline{\mathbf{x}}=\frac{1}{n} \mathbb{X}^{\prime} 1$
Donde $1$ representa un vector de unos de dimensión $n$ x $1$ para que la operación sea conformable.

```{r, message=FALSE, warning=FALSE}
v_unos=rep(1,n) #Generación del vector de unos
v_medias=(1/n)*t(PQM)%*%v_unos

kable(v_medias)
```

## 2. La matriz de observaciones centradas.

Para obtener la matriz de observaciones centradas es necesario realizar esta operación: $\tilde{\mathbb{X}}=\mathbb{X}-1 \overline{\mathbf{x}}^{\prime}$ donde $1 \overline{\mathbf{x}}^{\prime} =\frac{1}{n} 11^{\prime}\mathbb{X}$

```{r, warning=FALSE, message=FALSE}
PQM_centrada=PQM-(1/n)*(v_unos%*%t(v_unos))%*%(as.matrix(PQM))
kable(PQM_centrada)%>%
  kable_styling() %>%
  scroll_box(height="500px")
```

## 3. La matriz de covarianza muestral usando los vectores observados y la matriz de observaciones centradas.

Para obtener la matriz de covarianza muestral usando los vectores observados se propone la sumatoria $\mathbf{S}_{\mathbf{X},\mathbf{n}}=\frac{1}{n}\sum_{j=1}^n\left(\mathbf{x}_j-\overline{\mathbf{x}}\right)\left(\mathbf{x}_j-\overline{\mathbf{x}}\right)^{\prime}$ de donde se obtiene la matriz de covarianzas muestral multplicando la matriz obtenida por el siguiente factor: $\mathbf{S}_{\mathbf{X}}=\left(\frac{n}{n-1}\mathbf{S}_{\mathbf{X}, n}\right)$

```{r, message=FALSE, warning=FALSE}
cov=matrix(rep(0,p^2), ncol=p, byrow = TRUE)

for (i in c(1:n)){
  a=as.matrix(t(PQM[i,]-v_medias))%*%as.matrix((PQM[i,]-v_medias))
  cov=cov+a
}

cov=(1/n)*cov

covm=(n/(n-1))*cov
kable(covm)
```

Para obtener la matriz de covarianzas muestrales usando la matriz de observaciones centrada se realiza la siguiente operación: $\mathbf{S}_{\mathbf{X}}=\frac{1}{n-1}\tilde{\mathbb{X}}^{\prime}\tilde{\mathbb{X}}$

```{r, message=FALSE, warning=FALSE}
covm2=(1/(n-1))*t(as.matrix(PQM_centrada))%*%as.matrix(PQM_centrada)

kable(covm2)
```

## 4. Varianza total.

La varianza total se define cómo: $traza(\mathbf{S}_{\mathbf{X}})$

```{r, message=FALSE, warning=FALSE}
vartot=sum(diag(covm)) #La traza es la suma de los elementos de la diagonal
vartot
```

## 5. Varianza generalizada.

La varianza generalizada se define cómo: $det(\mathbf{S}_{\mathbf{X}}) = |\mathbf{S}_{\mathbf{X}}|$

```{r, message=FALSE, warning=FALSE}
det(covm)
```

## 6. La matriz de correlación muestral.

La matriz de correlación muestral se define cómo la matriz $\mathrm{R}=\left(r_{j k}\right)$ que contiene las correlaciones muestrales de las $p$ variables:\
$\mathbf{R}=\left(\begin{array}{cccc}1 & r_{12} & \cdots & r_{1 p} \\r_{21} & 1 & \cdots & r_{2 p} \\\vdots & \ddots & \vdots & \\r_{p 1} & r_{p 2} & \cdots & 1\end{array}\right)$\
El calculo de la matriz de correlaciones muestral es trivial en r con la función cor().

```{r, message=FALSE, warning=FALSE}
kable(cor(PQM))
```

## 7. La matriz de correlación muestral a partir de la matriz de covarianza muestral.

Usando la siguiente expresión podemos hallar la matriz de correlación muestral a partir de la matriz de covarianzas muestral: $\mathbf{R}=\mathbf{V}^{-\left(\frac{1}{2}\right)} \mathbf{S V}^{-\left(\frac{1}{2}\right)}$ donde $\mathbf{V}$ es la matriz desviación estándar y se define cómo:

$\mathbf{V}^{1 / 2}=\left(\begin{array}{cccc}\sqrt{s_{11}} & 0 & \cdots & 0 \\ 0 & \sqrt{s_{22}} & \cdots & 0 \\ \vdots & \ddots & \cdots & \vdots \\ 0 & 0 & \cdots & \sqrt{s_{p p}}\end{array}\right)$

```{r, message=FALSE, warning=FALSE}
sqrtcov=sqrt(diag(covm))*diag(3) #Matriz desviación estandar
cor2=solve(sqrtcov)%*%covm%*%solve(sqrtcov)
colnames(cor2)=colnames(cor(PQM))
rownames(cor2)=rownames(cor(PQM))
kable(cor2)
```

## 8. Los datos estandarizados (estandarización univariada y multivariada).

Para realizar la estandarización univariada se hace uso de la matriz desviación estándar, con esta se puede obtener los vectores $\mathbf{y}$ estandarizados como: $\mathbf{y}=\mathbf{V}^{-\frac{1}{2}}\left(\mathbf{x}-\overline{\mathbf{x}}\right)$

```{r, message=FALSE, warning=FALSE}
est1=as.matrix(PQM_centrada)%*%solve(sqrtcov)
colnames(est1)=colnames(PQM)
kable(est1)%>%
  kable_styling() %>%
  scroll_box(height="500px")
```

Para realizar la estandarización multivariada es necesario hallar la matriz raiz cuadrada de la matriz de covarianza muestrales. Recordando la descomposición espectral de la matriz de covarianzas $\mathbf{S}_{\mathbf{X}}=\mathbf{A}\mathbf{D}\mathbf{A}^{\prime}$ donde $\mathbf{D}$ es diagonal y contiene los valores propios de $\mathbf{S}_{\mathbf{X}}$ y $\mathbf{A}$ es ortogonal y contiene los
vectores propios. Por lo anterior $\mathbf{S}_{\mathbf{X}}^{\frac{1}{2}}$ se puede obtener como $\mathbf{S}_{\mathbf{X}}^{\frac{1}{2}}=\mathbf{A}\mathbf{D}^{\frac{1}{2}}\mathbf{A}^{\prime}$.

```{r, message=FALSE, warning=FALSE}
A=as.matrix(eigen(covm)$vectors)
D=eigen(covm)$values*diag(3)

A%*%D%*%t(A)  #descomposicion en valores espectrales
sqrtD=sqrt(D) #Para hallar S^(1/2)
sqrtS=A%*%sqrtD%*%t(A) #S^(1/2)

```

A partir de $\mathbf{S}_{\mathbf{X}}^{-\frac{1}{2}}$ se obtiene la estandarización multivariada de la forma $\mathbb{Y}=\tilde{\mathbb{X}}\mathbf{S}_\mathbf{X}^{-\frac{1}{2}}$

```{r, message=FALSE, warning=FALSE}
est.mul=as.matrix(PQM_centrada)%*%sqrtS

colnames(est.mul)=colnames(PQM)
kable(est.mul)%>%
  kable_styling() %>%
  scroll_box(height="500px")
```

## 9. El vector de medias muestral de los datos estandarizados.

Para hallar el vector de medias muestral de los datos estandarizados se puede calcular usando los vectores estandarizados mediante la siguiente formula: $\overline{\tilde{\mathbf{x}}}=\frac{1}{n} \sum_{j=1}^n \tilde{\mathbf{x}_j}$

```{r, message=FALSE, warning=FALSE}
est.mean=rep(0,p)

for (i in c(1:n)){
  a = est1[i,]
  est.mean=est.mean+a
}

est.mean=(1/n)*est.mean

est.mean=t(as.matrix(est.mean))

kable(est.mean)
```

## 10. La matriz de covarianza de los datos estandarizados.

Para hallar la matriz de covarianza de los datos estandarizados $\mathbf{S}_{\tilde{\mathbf{X}}}=\left(\frac{n}{n-1}\mathbf{S}_{\tilde{\mathbf{X}}, n}\right)$ donde$\mathbf{S}_{\tilde{\mathbf{X}},\mathbf{n}}=\frac{1}{n}\sum_{j=1}^n\left(\tilde{\mathbf{x}_j}-\overline{\tilde{\mathbf{x}}}\right)\left(\tilde{\mathbf{x}_j}-\overline{\tilde{\mathbf{x}}}\right)^{\prime}$

```{r, message=FALSE, warning=FALSE}
cov.est=matrix(rep(0,p^2), ncol=p, byrow = TRUE)

for (i in c(1:n)){
  a=as.matrix(t(est1[i,]-est.mean))%*%as.matrix((est1[i,]-est.mean))
  cov.est=cov.est+a
}

cov.est=(1/n)*cov.est

covm.est=(n/(n-1))*cov.est

kable(covm.est)
```

## 11. Determine gráficamente si los datos siguen una distribución normal multivariada.

```{r, message=FALSE, warning=FALSE}
dist.mah=rep(0,n)

for (i in c(1:n)){
  dist.mah[i]=(as.matrix(PQM_centrada[i,]))%*%solve(covm)%*%t(as.matrix(PQM_centrada[i,]))
} #Calculo manual de la distancia de Mahalanobis

chi2_t=rep(0,n)

for (i in c(1:n)){
  chi2_t[i]=qchisq((i-0.5)/n, df=p)
} #Calculo de los cuantiles teóricos

chi2_t=as.vector(chi2_t)
dist.mah=as.vector(dist.mah)

hist(dist.mah, freq = FALSE, ylim =c (0,0.25), xlab = "Mahalanobis distance")
curve(expr = dchisq(x, df=ncol(PQM)), from=0, to=35, add = TRUE, col="red")
legend("topright", legend = expression(chi^2~(3)), col="red", lty=1)

qqplot(chi2_t, dist.mah,
       main = "Chi-square plot",
       xlab = expression(chi^2 ~ "(3," ~ frac(j-frac(1,2), 41) ~ ")"),
       ylab = "Mahalanobis distances",
       cex.lab=0.7)
abline(0, 1, col = "red", lwd = 1) 


```

## 12. Aplique una prueba formal para determinar si los datos siguen una distribución normal p-variada.

```{r}
sum(dist.mah<=qchisq(.5,p))/n #Observar las proporciones

mvn(PQM, mvnTest ="royston" , desc=FALSE, univariateTest = "SW")

```

## 13. Encuentre la mediana.

```{r, message=FALSE, warning=FALSE}

PQM[which.max(depth(PQM, method = "Mahalanobis")),] #Mediana Mahalanobis

PQM[which.max(depth(PQM, method = "Tukey")),] #Mediana Tukey

```

## 14. Realice un bagplot e interprete sus resultados.

```{r, warning=FALSE, message=FALSE}
bagplot(PQM[,c(1,2)],
        xlab=colnames(PQM)[1],
        ylab=colnames(PQM)[2])

bagplot(PQM[,c(1,3)],
        xlab=colnames(PQM)[1],
        ylab=colnames(PQM)[3])

bagplot(PQM[,c(2,3)],
        xlab=colnames(PQM)[2],
        ylab=colnames(PQM)[3])

```

- El punto rojo en el centro del bagplot representa la mediana multivariada de los datos. Es el centro robusto de la distribución, calculado utilizando una medida como la profundidad de Tukey.

- Área interna (bag): El área azul más oscura abarca el 50\% de los datos centrales, lo que refleja la concentración principal de la distribución. Los puntos dentro de esta región se consideran datos normales y típicos.

- Área externa (fence): El área azul más clara incluye todos los puntos que no son considerados valores atípicos. Esta región define los límites dentro de los cuales se espera que se encuentren los datos bajo una distribución normal.

- Valores atípicos: Los puntos rojos fuera del área externa son considerados outliers. Estos valores se encuentran más allá del límite definido por el fence y podrían corresponder a observaciones inusuales o errores en los datos.

- Forma de la distribución: La forma general del bagplot proporciona información sobre la estructura y la orientación de la distribución. Si la forma es alargada, esto puede indicar correlaciones entre las variables. Una forma circular sugiere independencia y una distribución más uniforme.

- Orientación de los rayos (direcciones): Las líneas rojas que conectan la mediana multivariada con los límites exteriores ayudan a identificar la dirección de la dispersión en los datos. Estas líneas pueden indicar patrones específicos en la variabilidad de las observaciones.

El análisis detallado de cada bagplot permite identificar patrones en los datos, evaluar la homogeneidad de la distribución, y detectar puntos atípicos que podrían requerir mayor atención en el análisis estadístico.

## 15. Encuentre observaciones outliers.

a\. Usando el procedimiento clásico sugerido en el libro de Johnson, R. A., & Wichern, D. W. (2014)

Siguiendo la estrategia propuesta en el libro debemos seguir 4 pasos:

1.   Dotplots univariados:

    ```{r, warning=FALSE}
    ggplot(PQM, aes(x = Density)) + 
      geom_dotplot() +
      theme_minimal() 

    ggplot(PQM, aes(x = Machine.direction)) + 
      geom_dotplot() +
      theme_minimal() 

    ggplot(PQM, aes(x = Cross.direction)) + 
      geom_dotplot() +
      theme_minimal()

    ```

2.  Gráficos de dispersión bivariados:

    ```{r, warning=FALSE, message=FALSE}

    plot(PQM[,c(1,2)])
    plot(PQM[,c(1,3)])
    plot(PQM[,c(2,3)])
    ```

3.  Detectando observaciones para los datos estandarizados:

    ```{r, message=FALSE, warning=FALSE}
    est1_styled <- apply(est1, c(1, 2), function(x) {
      if (abs(as.numeric(x)) > 3.5) {
        # Resaltar observaciones estandarizadas superiores en valor absoluto a 3.5
        paste0("<b><span style='background-color:yellow'>", x, "</span></b>")
      } else {
        x
      }
    })
    est1_styled <- cbind(Index = rep(1:n), est1_styled)

    kable(est1_styled, "html", escape = FALSE) %>%
      kable_styling() %>%
      scroll_box(height="500px")
    ```

4.  Revisando valores de la distacia de Mahalanobis que superen al percentil 95 de la distribución chi-cuadrado:

    ```{r, message=FALSE, warning=FALSE}
    d.mah_styled <- sapply(dist.mah, function(x) {
      if (as.numeric(x) > qchisq(0.95, p)) {
        # Resaltar distancias al cuadrado que superen al percentil teórico
        paste0("<b><span style='background-color:yellow'>", x, "</span></b>")
      } else {
        x
      }
    })

    d.mah_styled=cbind(Index=rep(1:n),d.mah_styled)

    kable(as.data.frame(d.mah_styled), "html", escape = FALSE,col.names = c("Index", "$d^2$")) %>%
      kable_styling() %>%
      scroll_box(height="500px")
    ```

```{r include=FALSE}
X <- read_excel('Tabla12.xlsx', col_names = TRUE, range = cell_cols(2:4))
Xmatrix <- data.matrix(X)

n = nrow(X)
```



```{r include=FALSE}
par(mfrow=c(1,3))

stripchart(X[,1], method = "stack", offset = .5, at = 0, pch = 19, col = "steelblue", xlab = "Density")
stripchart(X[,2], method = "stack", offset = .5, at = 0, pch = 19, col = "steelblue", xlab = "Machine Direction")
stripchart(X[,3], method = "stack", offset = .5, at = 0, pch = 19, col = "steelblue", xlab = "Cross Direction")

```


```{r include=FALSE}
CovX <- cov(X)
xbarra = NULL

for (j in 1:3) {
  xbarra[j] <- (1/41)*sum(X[,j])
}

SDmatrix <- matrix(c(sqrt(CovX[1,1]), 0, 0,
                     0, sqrt(CovX[2,2]), 0,
                     0, 0, sqrt(CovX[3,3])), ncol = 3, nrow = 3)

stand <- matrix(NA, nrow = 3, ncol = 41)

restar <- function(vector) vector - xbarra


uni_stand <-(solve(SDmatrix)) %*% apply(X, 1, restar)
uni_stand <- t(uni_stand)

dist <- NULL

for (i in 1:n) {
  dist[i] <- t(Xmatrix[i,] - colMeans(X)) %*% solve(var(X)) %*% (Xmatrix[i,] - colMeans(X))
}

```



b. Usando la distancia euclidiana robusta

Si en la distancia euclidiana se toma la mediana como estimador robusto de la media, las distancias son de esta forma:

```{r}
eucl_rob <- NULL

for (j in 1:n) {
  eucl_rob[j] <- sqrt(sum((Xmatrix[j,] - Xmatrix[13,])^2))
}
```
Y buscaríamos por valores muy grandes o muy pequeños.

```{r}
Eucl_rob <- data.frame(Observación = 1:n, X, Distancia_euclidiana_robusta = eucl_rob)

kable(Eucl_rob)
```

```{r}
kable(sqldf("select *
             from Eucl_rob
             order by Distancia_euclidiana_robusta desc"))
```
Y los valores más grandes corresponden a las primeras observaciones que se pueden ver en la tabla.

```{r}
kable(sqldf("select *
             from Eucl_rob
             where Distancia_euclidiana_robusta > 0
             order by Distancia_euclidiana_robusta asc
             "))
```


Los valores más pequeños son los primeros que aparecen en la tabla.

En las dos tablas, los primeros valores se pueden considerar outliers según la distancia euclidiana robusta.


c. Usando el concepto de profundidad


```{r}
Dmah = NULL

for (j in 1:n) {
  Dmah[j] <- 1/(1+ (t(Xmatrix[j,] - xbarra) %*% solve(CovX)  %*% (Xmatrix[j,] - xbarra)))
}

Prof <- data.frame(1:n, X, Profundidad = Dmah)

```
Y buscamos por los valores más bajos pues estos serían los menos profundos y así, candidatos a outliers.

```{r}
kable(sqldf("select *
       from Prof
       order by Profundidad asc"))
```
Bajo la función de profundidad basada en la distancia de Mahalanobis, los posibles outliers corresponden a las primeras observaciones en la tabla anterior. Por ejemplo, la observación 25 era la misma que habíamos catalogado como outlier anteriormente. 

## 16
Realice e interprete el gráfico DD plot

```{r echo=F}
f <- Xmatrix[1:20,]
  
g <- Xmatrix[22:n,]


depths_f <- depth(f, f)
depths_g <- depth(g, g)


library(ggplot2)
df <- data.frame(Depth1 = depths_f, Depth2 = depths_g)
ggplot(df, aes(x = Depth1, y = Depth2)) +
  geom_point(color = "blue", size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  labs(title = "DD-Plot", x = "Depth w.r.t. Data 1", y = "Depth w.r.t. Data 2") +
  theme_minimal()


```

**Puntos por encima de la línea :**  
  Esto indica que estos puntos tienen mayor profundidad respecto al segundo conjunto (en el eje \(y\)) que respecto al primero (en el eje \(x\)).

**Puntos por debajo de la línea :**  
  Esto indica que estos puntos son más profundos respecto al primer conjunto (en el eje \(x\)).

**Puntos muy alejados de la línea **  
  Estos puntos podrían ser atípicos o tener comportamientos distintos respecto a los dos conjuntos.
